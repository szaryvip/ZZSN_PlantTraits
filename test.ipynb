{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from torcheval.metrics import R2Score\n",
    "from torchmetrics.functional import r2_score\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "import shutil\n",
    "import os\n",
    "from dataset import PGLSDataset\n",
    "from models import PGLSModel, EnsemblePGLSModel\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in [\"data/train_images\", \"data/test_images\"]:\n",
    "    if \"0\" not in os.listdir(folder):\n",
    "        print(\"Moving images to 0 folder\")\n",
    "        os.makedirs(f\"{folder}/0\")\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.lower().endswith(\".jpeg\"):\n",
    "                source_path = os.path.join(folder, filename)\n",
    "                target_path = os.path.join(f\"{folder}/0\", filename)\n",
    "\n",
    "                shutil.move(source_path, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to 224x224\n",
    "    transforms.ToTensor(),         # Convert PIL image to tensor (H x W x C) in the range [0.0, 1.0]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "batch_size = 16     # use 4 if problem with GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = 'data/train_images'\n",
    "test_images_path = 'data/test_images'\n",
    "\n",
    "train_csv_path = 'data/train.csv'\n",
    "test_csv_path = 'data/test.csv'\n",
    "\n",
    "\n",
    "tabular_data = pd.read_csv(train_csv_path)\n",
    "targets = [\"X4\", \"X11\", \"X18\", \"X26\", \"X50\", \"X3112\"]\n",
    "\n",
    "# Filter data\n",
    "upper_values = {}\n",
    "for target in targets:\n",
    "    upper_values[target] = tabular_data[target+\"_mean\"].quantile(0.99)\n",
    "    tabular_data = tabular_data[tabular_data[target+\"_mean\"] < upper_values[target]]\n",
    "    tabular_data = tabular_data[tabular_data[target+\"_mean\"] > 0]\n",
    "\n",
    "# Normalize the targets\n",
    "original_means = tabular_data[[f\"{target}_mean\" for target in targets]].mean()\n",
    "original_stds = tabular_data[[f\"{target}_mean\" for target in targets]].std()\n",
    "tabular_data[[f\"{target}_mean\" for target in targets]] = (tabular_data[[f\"{target}_mean\" for target in targets]] - original_means) / original_stds\n",
    "\n",
    "# Normalize the features\n",
    "tabular_input_size = 0\n",
    "for column in tabular_data.columns:\n",
    "    if column in [\"id\"]+[target+\"_mean\" for target in targets]+[target+\"_sd\" for target in targets]:\n",
    "        continue\n",
    "    tabular_input_size += 1\n",
    "    min_val = tabular_data[column].min()\n",
    "    max_val = tabular_data[column].max()\n",
    "    tabular_data[column] = (tabular_data[column] - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "test_tabular_data = pd.read_csv(test_csv_path)\n",
    "# Normalize the features\n",
    "for column in test_tabular_data.columns:\n",
    "    if column in [\"id\"]:\n",
    "        continue\n",
    "    min_val = test_tabular_data[column].min()\n",
    "    max_val = test_tabular_data[column].max()\n",
    "    test_tabular_data[column] = (test_tabular_data[column] - min_val) / (max_val - min_val)\n",
    "\n",
    "train_images_dataset = ImageFolder(root=train_images_path, transform=transform)\n",
    "test_images_dataset = ImageFolder(root=test_images_path, transform=transform)\n",
    "\n",
    "train_image_csv_dataset = PGLSDataset(tabular_data=tabular_data, image_folder=train_images_dataset, transform_csv=None)\n",
    "train, val = random_split(train_image_csv_dataset, [int(0.8*len(train_image_csv_dataset)), len(train_image_csv_dataset) - int(0.8*len(train_image_csv_dataset))])\n",
    "test_image_csv_dataset = PGLSDataset(tabular_data=test_tabular_data, image_folder=test_images_dataset, transform_csv=None)\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_image_csv_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_stds = torch.from_numpy(original_stds.values).float()\n",
    "original_means = torch.from_numpy(original_means.values).float()\n",
    "def denormalize_targets(targets):\n",
    "    return targets * original_stds + original_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effnet = efficientnet_b0(weights=EfficientNet_B0_Weights)\n",
    "effnet = timm.create_model(\n",
    "    'efficientnet_b4.ra2_in1k',\n",
    "    pretrained=True,\n",
    "    num_classes=0,\n",
    ")\n",
    "\n",
    "model = PGLSModel(effnet, tabular_input_size)\n",
    "metric = R2Score()\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "0.38923177123069763\n",
      "tensor(-0.5584)\n",
      "0.8054404258728027\n",
      "tensor(-0.0937)\n",
      "1.4053128957748413\n",
      "tensor(-0.0253)\n",
      "1.4774439334869385\n",
      "tensor(-0.0212)\n",
      "1.0113937854766846\n",
      "tensor(-0.0174)\n",
      "0.8494255542755127\n",
      "tensor(-0.0107)\n",
      "1.0444377660751343\n",
      "tensor(-0.0076)\n",
      "0.8111939430236816\n",
      "tensor(-0.0077)\n",
      "0.6304029226303101\n",
      "tensor(-0.0102)\n",
      "1.3158304691314697\n",
      "tensor(-0.0085)\n",
      "0.6532130837440491\n",
      "tensor(-0.0076)\n",
      "1.1371471881866455\n",
      "tensor(-0.0075)\n",
      "1.2428226470947266\n",
      "tensor(-0.0053)\n",
      "1.3151946067810059\n",
      "tensor(-0.0054)\n",
      "0.7168926000595093\n",
      "tensor(-0.0056)\n",
      "0.4788459241390228\n",
      "tensor(-0.0054)\n",
      "1.1843985319137573\n",
      "tensor(-0.0062)\n",
      "0.792550265789032\n",
      "tensor(-0.0060)\n",
      "1.410291075706482\n",
      "tensor(-0.0058)\n",
      "0.7021980285644531\n",
      "tensor(-0.0053)\n",
      "0.704716682434082\n",
      "tensor(-0.0043)\n",
      "1.379739761352539\n",
      "tensor(-0.0047)\n",
      "1.3159184455871582\n",
      "tensor(-0.0050)\n",
      "2.046245574951172\n",
      "tensor(-0.0051)\n",
      "1.2432420253753662\n",
      "tensor(-0.0038)\n",
      "0.8464863300323486\n",
      "tensor(-0.0029)\n",
      "0.6654527187347412\n",
      "tensor(-0.0023)\n",
      "0.34922027587890625\n",
      "tensor(-0.0017)\n",
      "0.550697386264801\n",
      "tensor(-0.0015)\n",
      "0.7228361964225769\n",
      "tensor(-0.0006)\n",
      "0.7839243412017822\n",
      "tensor(-0.0002)\n",
      "0.9151296615600586\n",
      "tensor(0.0004)\n",
      "1.0863919258117676\n",
      "tensor(0.0007)\n",
      "1.9851057529449463\n",
      "tensor(0.0008)\n",
      "0.6082032918930054\n",
      "tensor(0.0011)\n",
      "0.4603668451309204\n",
      "tensor(0.0017)\n",
      "0.8905223608016968\n",
      "tensor(0.0022)\n",
      "0.7373506426811218\n",
      "tensor(0.0024)\n",
      "0.647761344909668\n",
      "tensor(0.0031)\n",
      "0.9573413729667664\n",
      "tensor(0.0032)\n",
      "0.8299612402915955\n",
      "tensor(0.0033)\n",
      "2.203061103820801\n",
      "tensor(0.0037)\n",
      "0.7862586975097656\n",
      "tensor(0.0040)\n",
      "1.6131339073181152\n",
      "tensor(0.0039)\n",
      "1.204216480255127\n",
      "tensor(0.0043)\n",
      "0.778547465801239\n",
      "tensor(0.0046)\n",
      "0.5041401386260986\n",
      "tensor(0.0051)\n",
      "0.7939852476119995\n",
      "tensor(0.0053)\n",
      "1.27519953250885\n",
      "tensor(0.0053)\n",
      "0.7255668640136719\n",
      "tensor(0.0059)\n",
      "0.6212854385375977\n",
      "tensor(0.0069)\n",
      "1.8868846893310547\n",
      "tensor(0.0072)\n",
      "1.302276849746704\n",
      "tensor(0.0079)\n",
      "0.84792160987854\n",
      "tensor(0.0087)\n",
      "0.48807740211486816\n",
      "tensor(0.0088)\n",
      "0.7685465216636658\n",
      "tensor(0.0095)\n",
      "0.5779544115066528\n",
      "tensor(0.0100)\n",
      "1.7973487377166748\n",
      "tensor(0.0105)\n",
      "1.6782777309417725\n",
      "tensor(0.0111)\n",
      "1.1119554042816162\n",
      "tensor(0.0110)\n",
      "1.0972145795822144\n",
      "tensor(0.0104)\n",
      "1.9033397436141968\n",
      "tensor(0.0107)\n",
      "0.9114363193511963\n",
      "tensor(0.0108)\n",
      "0.727132260799408\n",
      "tensor(0.0112)\n",
      "1.6402109861373901\n",
      "tensor(0.0115)\n",
      "0.32595497369766235\n",
      "tensor(0.0117)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(new_max_value_metrics_batch \u001b[38;5;241m>\u001b[39m max_value_metrics_batch):\n\u001b[1;32m     31\u001b[0m         max_value_metrics_batch \u001b[38;5;241m=\u001b[39m new_max_value_metrics_batch\n\u001b[0;32m---> 32\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_best_batch_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(new_max_value_metrics_batch \u001b[38;5;241m>\u001b[39m max_value_metrics_epoch):\n\u001b[1;32m     35\u001b[0m     max_value_metrics_epoch \u001b[38;5;241m=\u001b[39m new_max_value_metrics_batch\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 628\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:859\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# given that we copy things around anyway, we might use storage.cpu()\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# this means to that to get tensors serialized, you need to implement\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;66;03m# .cpu() on the underlying Storage\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m storage\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 859\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    861\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/storage.py:137\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),maximize=False, lr=1e-4, weight_decay=1e-5)\n",
    "max_value_metrics_batch = float(\"-inf\")\n",
    "model_best_batch_path = \"./models/effnet_best_batch.pt\"\n",
    "max_value_metrics_epoch = float(\"-inf\")\n",
    "model_best_epoch_path = \"./models/effnet_best_epoch.pt\"\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    for data in train_data_loader:\n",
    "        image, features, targets = data\n",
    "        image = image.to(device)\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image, features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        outputs = outputs.to(\"cpu\")\n",
    "        targets = targets.to(\"cpu\")\n",
    "        outputs_denorm = denormalize_targets(outputs)\n",
    "        targets_denorm = denormalize_targets(targets)\n",
    "        metric.update(outputs_denorm, targets_denorm)\n",
    "        print(loss.item())\n",
    "        print(metric.compute())\n",
    "        new_max_value_metrics_batch = metric.compute().item()\n",
    "\n",
    "        if(new_max_value_metrics_batch > max_value_metrics_batch):\n",
    "            max_value_metrics_batch = new_max_value_metrics_batch\n",
    "            torch.save(model.state_dict(), model_best_batch_path)\n",
    "    \n",
    "    if(new_max_value_metrics_batch > max_value_metrics_epoch):\n",
    "        max_value_metrics_epoch = new_max_value_metrics_batch\n",
    "        torch.save(model.state_dict(), model_best_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on validation set:  0.8942889505368801\n",
      "Average R2 on validation set:  0.08000875921474278\n"
     ]
    }
   ],
   "source": [
    "accumulated_loss = 0\n",
    "iterations = 0\n",
    "accumulated_r2 = 0\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in val_data_loader:\n",
    "        image, features, targets = data\n",
    "        image = image.to(device)\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(image, features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        accumulated_loss += loss.item()\n",
    "        iterations += 1\n",
    "        outputs = outputs.to(\"cpu\")\n",
    "        targets = targets.to(\"cpu\")\n",
    "        outputs_denorm = denormalize_targets(outputs)\n",
    "        targets_denorm = denormalize_targets(targets)\n",
    "        metric.update(outputs_denorm, targets_denorm)\n",
    "        accumulated_r2 += metric.compute().item()\n",
    "print(\"Average loss on validation set: \", accumulated_loss/iterations)\n",
    "print(\"Average R2 on validation set: \", accumulated_r2/iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "model.eval()\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    for data in test_data_loader:\n",
    "        image, features, targets = data\n",
    "        image = image.to(device)\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(image, features)\n",
    "        outputs = outputs.to(\"cpu\")\n",
    "        outputs_denorm = denormalize_targets(outputs)\n",
    "        predictions.append(outputs_denorm)\n",
    "predictions = [item for sublist in predictions for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_csv_dataframe, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"id,X4,X11,X18,X26,X50,X3112\\n\")\n",
    "        for pred, id in zip(predictions, test_csv_dataframe[\"id\"]):\n",
    "            pred = [p.item() for p in pred]\n",
    "            f.write(f\"{id},{','.join([str(p) for p in pred])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions(predictions, test_tabular_data, \"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effnet = efficientnet_b0(weights=EfficientNet_B0_Weights)\n",
    "effnet = timm.create_model(\n",
    "    'efficientnet_b0.ra_in1k',\n",
    "    pretrained=True,\n",
    "    num_classes=0,\n",
    ")\n",
    "xception = timm.create_model('inception_resnet_v2.tf_in1k', pretrained=True, num_classes=0)\n",
    "densenet = timm.create_model('densenet121.ra_in1k', pretrained=True, num_classes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metric = R2Score()\n",
    "criterion = torch.nn.MSELoss()\n",
    "# criterion = r2_score\n",
    "ensemble_model = EnsemblePGLSModel([effnet, xception, densenet], tabular_input_size)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ZZSN/ZZSN_PlantTraits/models.py:42\u001b[0m, in \u001b[0;36mPGLSModel.forward\u001b[0;34m(self, image, tabular)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, tabular):\n\u001b[0;32m---> 42\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((image_features, tabular), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(features)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/timm/models/efficientnet.py:179\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 179\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/timm/models/efficientnet.py:162\u001b[0m, in \u001b[0;36mEfficientNet.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 162\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_stem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "ensemble_model.to(device)\n",
    "max_value_metrics_batch = float(\"-inf\")\n",
    "model_best_batch_path = \"./models/ensemble_best_batch.pt\"\n",
    "max_value_metrics_epoch = float(\"-inf\")\n",
    "model_best_epoch_path = \"./models/ensemble_best_epoch.pt\"\n",
    "optimizer = torch.optim.Adam(ensemble_model.parameters(), lr=1e-4, weight_decay=1e-5, maximize=False)\n",
    "\n",
    "ensemble_model.train()\n",
    "for epoch in range(2):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    for data in train_data_loader:\n",
    "        image, features, targets = data\n",
    "        image = image.to(device)\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image, features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        outputs = outputs.to(\"cpu\")\n",
    "        targets = targets.to(\"cpu\")\n",
    "        outputs_denorm = denormalize_targets(outputs)\n",
    "        targets_denorm = denormalize_targets(targets)\n",
    "        metric.update(outputs_denorm, targets_denorm)\n",
    "        print(loss.item())\n",
    "        print(metric.compute())\n",
    "        new_max_value_metrics_batch = metric.compute().item()\n",
    "\n",
    "        if(new_max_value_metrics_batch > max_value_metrics_batch):\n",
    "            max_value_metrics_batch = new_max_value_metrics_batch\n",
    "            torch.save(model.state_dict(), model_best_batch_path)\n",
    "    \n",
    "    if(new_max_value_metrics_batch > max_value_metrics_epoch):\n",
    "        max_value_metrics_epoch = new_max_value_metrics_batch\n",
    "        torch.save(model.state_dict(), model_best_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
