{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from torcheval.metrics import R2Score\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "import shutil\n",
    "import os\n",
    "from dataset import PGLSDataset\n",
    "from models import PGLSModel, SimpleTabularModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in [\"data/train_images\", \"data/test_images\"]:\n",
    "    if \"0\" not in os.listdir(folder):\n",
    "        print(\"Moving images to 0 folder\")\n",
    "        os.makedirs(f\"{folder}/0\")\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.lower().endswith(\".jpeg\"):\n",
    "                source_path = os.path.join(folder, filename)\n",
    "                target_path = os.path.join(f\"{folder}/0\", filename)\n",
    "\n",
    "                shutil.move(source_path, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to 224x224\n",
    "    transforms.ToTensor()            # Convert PIL image to tensor (H x W x C) in the range [0.0, 1.0]\n",
    "])\n",
    "\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = 'data/train_images'\n",
    "test_images_path = 'data/test_images'\n",
    "\n",
    "train_csv_path = 'data/train.csv'\n",
    "test_csv_path = 'data/test.csv'\n",
    "\n",
    "\n",
    "tabular_data = pd.read_csv(train_csv_path)\n",
    "targets = [\"X4\", \"X11\", \"X18\", \"X26\", \"X50\", \"X3112\"]\n",
    "\n",
    "# Filter data\n",
    "upper_values = {}\n",
    "for target in targets:\n",
    "    upper_values[target] = tabular_data[target+\"_mean\"].quantile(0.99)\n",
    "    tabular_data = tabular_data[tabular_data[target+\"_mean\"] < upper_values[target]]\n",
    "    tabular_data = tabular_data[tabular_data[target+\"_mean\"] > 0]\n",
    "\n",
    "# Normalize the targets\n",
    "original_means = tabular_data[[f\"{target}_mean\" for target in targets]].mean()\n",
    "original_stds = tabular_data[[f\"{target}_mean\" for target in targets]].std()\n",
    "tabular_data[[f\"{target}_mean\" for target in targets]] = (tabular_data[[f\"{target}_mean\" for target in targets]] - original_means) / original_stds\n",
    "\n",
    "# Normalize the features\n",
    "tabular_input_size = 0\n",
    "for column in tabular_data.columns:\n",
    "    if column in [\"id\"]+[target+\"_mean\" for target in targets]+[target+\"_sd\" for target in targets]:\n",
    "        continue\n",
    "    tabular_input_size += 1\n",
    "    min_val = tabular_data[column].min()\n",
    "    max_val = tabular_data[column].max()\n",
    "    tabular_data[column] = (tabular_data[column] - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "test_tabular_data = pd.read_csv(test_csv_path)\n",
    "# Normalize the features\n",
    "for column in test_tabular_data.columns:\n",
    "    if column in [\"id\"]:\n",
    "        continue\n",
    "    min_val = test_tabular_data[column].min()\n",
    "    max_val = test_tabular_data[column].max()\n",
    "    test_tabular_data[column] = (test_tabular_data[column] - min_val) / (max_val - min_val)\n",
    "\n",
    "train_images_dataset = ImageFolder(root=train_images_path, transform=transform)\n",
    "test_images_dataset = ImageFolder(root=test_images_path, transform=transform)\n",
    "\n",
    "train_image_csv_dataset = PGLSDataset(tabular_data=tabular_data, image_folder=train_images_dataset, transform_csv=None)\n",
    "train, val = random_split(train_image_csv_dataset, [int(0.8*len(train_image_csv_dataset)), len(train_image_csv_dataset) - int(0.8*len(train_image_csv_dataset))])\n",
    "test_image_csv_dataset = PGLSDataset(tabular_data=test_tabular_data, image_folder=test_images_dataset, transform_csv=None)\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_image_csv_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_stds = torch.from_numpy(original_stds.values).float().to(device)\n",
    "original_means = torch.from_numpy(original_means.values).float().to(device)\n",
    "def denormalize_targets(targets):\n",
    "    return targets * original_stds + original_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szaryvip/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "effnet = efficientnet_b0(weights=EfficientNet_B0_Weights)\n",
    "tabular_model = SimpleTabularModel(input_data_len=tabular_input_size)\n",
    "model = PGLSModel(effnet, tabular_model)\n",
    "metric = R2Score()\n",
    "criterion = torch.nn.MSELoss()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "1.5553114414215088\n",
      "tensor(-0.1723, device='cuda:0')\n",
      "1.7458369731903076\n",
      "tensor(-0.1742, device='cuda:0')\n",
      "1.6123594045639038\n",
      "tensor(-0.1756, device='cuda:0')\n",
      "0.8083957433700562\n",
      "tensor(-0.1760, device='cuda:0')\n",
      "1.3745969533920288\n",
      "tensor(-0.1755, device='cuda:0')\n",
      "2.9476382732391357\n",
      "tensor(-0.1720, device='cuda:0')\n",
      "1.6184685230255127\n",
      "tensor(-0.1738, device='cuda:0')\n",
      "1.263737678527832\n",
      "tensor(-0.1745, device='cuda:0')\n",
      "0.964460015296936\n",
      "tensor(-0.1758, device='cuda:0')\n",
      "1.1087892055511475\n",
      "tensor(-0.1761, device='cuda:0')\n",
      "0.6975414156913757\n",
      "tensor(-0.1753, device='cuda:0')\n",
      "2.152010679244995\n",
      "tensor(-0.1785, device='cuda:0')\n",
      "1.0030863285064697\n",
      "tensor(-0.1791, device='cuda:0')\n",
      "0.9457761645317078\n",
      "tensor(-0.1788, device='cuda:0')\n",
      "0.8785809278488159\n",
      "tensor(-0.1800, device='cuda:0')\n",
      "1.2756803035736084\n",
      "tensor(-0.1806, device='cuda:0')\n",
      "0.5454398989677429\n",
      "tensor(-0.1795, device='cuda:0')\n",
      "1.347793459892273\n",
      "tensor(-0.1826, device='cuda:0')\n",
      "1.5428683757781982\n",
      "tensor(-0.1837, device='cuda:0')\n",
      "3.8449294567108154\n",
      "tensor(-0.1842, device='cuda:0')\n",
      "0.5442394018173218\n",
      "tensor(-0.1849, device='cuda:0')\n",
      "1.0254428386688232\n",
      "tensor(-0.1853, device='cuda:0')\n",
      "2.3515753746032715\n",
      "tensor(-0.1803, device='cuda:0')\n",
      "0.37749797105789185\n",
      "tensor(-0.1806, device='cuda:0')\n",
      "0.5983241200447083\n",
      "tensor(-0.1806, device='cuda:0')\n",
      "0.6761800646781921\n",
      "tensor(-0.1807, device='cuda:0')\n",
      "0.8952339887619019\n",
      "tensor(-0.1819, device='cuda:0')\n",
      "1.729986548423767\n",
      "tensor(-0.1831, device='cuda:0')\n",
      "0.4851311445236206\n",
      "tensor(-0.1835, device='cuda:0')\n",
      "0.6823642253875732\n",
      "tensor(-0.1835, device='cuda:0')\n",
      "2.0435023307800293\n",
      "tensor(-0.1825, device='cuda:0')\n",
      "1.1630209684371948\n",
      "tensor(-0.1823, device='cuda:0')\n",
      "0.3492676019668579\n",
      "tensor(-0.1822, device='cuda:0')\n",
      "0.7663518786430359\n",
      "tensor(-0.1827, device='cuda:0')\n",
      "1.5212352275848389\n",
      "tensor(-0.1831, device='cuda:0')\n",
      "1.7111071348190308\n",
      "tensor(-0.1836, device='cuda:0')\n",
      "0.9038904905319214\n",
      "tensor(-0.1838, device='cuda:0')\n",
      "2.346846580505371\n",
      "tensor(-0.1790, device='cuda:0')\n",
      "0.9753787517547607\n",
      "tensor(-0.1791, device='cuda:0')\n",
      "1.8392043113708496\n",
      "tensor(-0.1797, device='cuda:0')\n",
      "1.578779935836792\n",
      "tensor(-0.1788, device='cuda:0')\n",
      "0.5171323418617249\n",
      "tensor(-0.1793, device='cuda:0')\n",
      "0.7385832667350769\n",
      "tensor(-0.1803, device='cuda:0')\n",
      "0.5395879149436951\n",
      "tensor(-0.1809, device='cuda:0')\n",
      "0.4228897988796234\n",
      "tensor(-0.1808, device='cuda:0')\n",
      "4.626110076904297\n",
      "tensor(-0.1774, device='cuda:0')\n",
      "1.190434217453003\n",
      "tensor(-0.1757, device='cuda:0')\n",
      "0.8620604276657104\n",
      "tensor(-0.1744, device='cuda:0')\n",
      "1.6016931533813477\n",
      "tensor(-0.1733, device='cuda:0')\n",
      "1.8247318267822266\n",
      "tensor(-0.1746, device='cuda:0')\n",
      "1.2260733842849731\n",
      "tensor(-0.1734, device='cuda:0')\n",
      "1.3295166492462158\n",
      "tensor(-0.1747, device='cuda:0')\n",
      "2.208287239074707\n",
      "tensor(-0.1724, device='cuda:0')\n",
      "1.106705904006958\n",
      "tensor(-0.1720, device='cuda:0')\n",
      "0.6018019914627075\n",
      "tensor(-0.1723, device='cuda:0')\n",
      "0.4138791561126709\n",
      "tensor(-0.1723, device='cuda:0')\n",
      "1.2518925666809082\n",
      "tensor(-0.1731, device='cuda:0')\n",
      "0.846104621887207\n",
      "tensor(-0.1723, device='cuda:0')\n",
      "0.6819543838500977\n",
      "tensor(-0.1729, device='cuda:0')\n",
      "1.0685113668441772\n",
      "tensor(-0.1730, device='cuda:0')\n",
      "1.2624022960662842\n",
      "tensor(-0.1734, device='cuda:0')\n",
      "0.39558252692222595\n",
      "tensor(-0.1732, device='cuda:0')\n",
      "0.9631511569023132\n",
      "tensor(-0.1721, device='cuda:0')\n",
      "1.4782559871673584\n",
      "tensor(-0.1704, device='cuda:0')\n",
      "0.2944089472293854\n",
      "tensor(-0.1701, device='cuda:0')\n",
      "0.8016294836997986\n",
      "tensor(-0.1697, device='cuda:0')\n",
      "0.7036185264587402\n",
      "tensor(-0.1691, device='cuda:0')\n",
      "0.39912840723991394\n",
      "tensor(-0.1685, device='cuda:0')\n",
      "0.8900262713432312\n",
      "tensor(-0.1684, device='cuda:0')\n",
      "0.5048805475234985\n",
      "tensor(-0.1689, device='cuda:0')\n",
      "1.142167091369629\n",
      "tensor(-0.1676, device='cuda:0')\n",
      "0.656182050704956\n",
      "tensor(-0.1672, device='cuda:0')\n",
      "1.061775803565979\n",
      "tensor(-0.1674, device='cuda:0')\n",
      "0.5049198269844055\n",
      "tensor(-0.1671, device='cuda:0')\n",
      "1.0596392154693604\n",
      "tensor(-0.1670, device='cuda:0')\n",
      "1.4294956922531128\n",
      "tensor(-0.1684, device='cuda:0')\n",
      "0.31058937311172485\n",
      "tensor(-0.1682, device='cuda:0')\n",
      "0.24184630811214447\n",
      "tensor(-0.1678, device='cuda:0')\n",
      "1.0393811464309692\n",
      "tensor(-0.1671, device='cuda:0')\n",
      "0.6698405146598816\n",
      "tensor(-0.1662, device='cuda:0')\n",
      "1.4988985061645508\n",
      "tensor(-0.1661, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_data_loader:\n\u001b[1;32m      9\u001b[0m         image, features, targets \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     10\u001b[0m         image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/ZZSN/ZZSN_PlantTraits/dataset.py:27\u001b[0m, in \u001b[0;36mPGLSDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_frame\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 27\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_folder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_folder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/0/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.jpeg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_folder\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_folder\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/folder.py:264\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    263\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/PIL/Image.py:889\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[1;32m    848\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/PIL/ImageFile.py:253\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    252\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 253\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    for data in train_data_loader:\n",
    "        image, features, targets = data\n",
    "        image = image.to(device)\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image, features)\n",
    "        outputs_denorm = denormalize_targets(outputs)\n",
    "        targets_denorm = denormalize_targets(targets)\n",
    "        loss = criterion(outputs, targets)\n",
    "        metric.update(outputs_denorm, targets_denorm)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "        print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on validation set:  0.869040786021616\n",
      "Average R2 on validation set:  0.09196601509144779\n"
     ]
    }
   ],
   "source": [
    "accumulated_loss = 0\n",
    "iterations = 0\n",
    "accumulated_r2 = 0\n",
    "model.to(device)\n",
    "model.eval()\n",
    "for data in val_data_loader:\n",
    "    image, features, targets = data\n",
    "    image = image.to(device)\n",
    "    features = features.to(device)\n",
    "    targets = targets.to(device)\n",
    "    outputs = model(image, features)\n",
    "    loss = criterion(outputs, targets)\n",
    "    accumulated_loss += loss.item()\n",
    "    iterations += 1\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "    outputs_denorm = torch.from_numpy(denormalize_targets(outputs))\n",
    "    targets_denorm = torch.from_numpy(denormalize_targets(targets))\n",
    "    metric.update(outputs_denorm, targets_denorm)\n",
    "    accumulated_r2 += metric.compute().item()\n",
    "print(\"Average loss on validation set: \", accumulated_loss/iterations)\n",
    "print(\"Average R2 on validation set: \", accumulated_r2/iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "model.eval()\n",
    "model.to(device)\n",
    "for data in test_data_loader:\n",
    "    image, features, targets = data\n",
    "    image = image.to(device)\n",
    "    features = features.to(device)\n",
    "    targets = targets.to(device)\n",
    "    outputs = model(image, features)\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "    outputs_denorm = denormalize_targets(outputs)\n",
    "    predictions.append(outputs_denorm)\n",
    "predictions = [item for sublist in predictions for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(predictions, test_csv_dataframe, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"id,X4,X11,X18,X26,X50,X3112\\n\")\n",
    "        for pred, id in zip(predictions, test_csv_dataframe[\"id\"]):\n",
    "            f.write(f\"{id},{','.join([str(p) for p in pred])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions(predictions, test_tabular_data, \"predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
